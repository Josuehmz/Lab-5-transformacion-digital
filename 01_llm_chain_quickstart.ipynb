{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c30b37b5",
      "metadata": {},
      "source": [
        "# Notebook 01 - Pinecone Vector Store Integration with LangChain\n",
        "\n",
        "In this notebook I follow the official [Pinecone integration tutorial](https://python.langchain.com/docs/integrations/vectorstores/pinecone) from the LangChain documentation.\n",
        "\n",
        "**What I will cover:**\n",
        "1. **Setup** - I configure API credentials (Google Gemini + Pinecone).\n",
        "2. **Initialization** - I create (or connect to) a Pinecone index and initialize the vector store.\n",
        "3. **Manage the vector store** - I add and delete documents.\n",
        "4. **Query the vector store** - I perform similarity search, similarity search with score, and use a retriever.\n",
        "\n",
        "> **Note:** I use **Google Gemini** (free tier) instead of OpenAI for embeddings. API keys are loaded from environment variables or prompted via `getpass`. I never hard-code secrets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb04bd82",
      "metadata": {},
      "source": [
        "## 1 - Setup\n",
        "\n",
        "I load environment variables from a `.env` file (if present) and make sure both `GOOGLE_API_KEY` and `PINECONE_API_KEY` are available.\n",
        "\n",
        "- **Google Gemini** provides free-tier access to embedding models (`gemini-embedding-001`, 768 dims via MRL) and chat models.\n",
        "- **Pinecone** offers a free starter tier for vector storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "270643df",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
        "\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e7d7c0",
      "metadata": {},
      "source": [
        "### Connect to Pinecone\n",
        "\n",
        "I create a `Pinecone` client using the API key. This client lets me manage indexes (create, list, delete) and obtain `Index` handles for reading/writing vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1686075b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b5ff17d",
      "metadata": {},
      "source": [
        "## 2 - Initialization\n",
        "\n",
        "Before initializing the LangChain vector store I need a Pinecone **index**. If one with the chosen name does not exist yet, I create it as a *serverless* index with:\n",
        "- **dimension = 768** - I use `gemini-embedding-001` with `output_dimensionality=768` (Matryoshka Representation Learning allows flexible sizing).\n",
        "- **metric = cosine** (standard for semantic similarity).\n",
        "- Hosted on **AWS us-east-1**.\n",
        "\n",
        "If the index already exists with a different dimension, I delete and recreate it automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "10277476",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name = \"langchain-test-index\"\n",
        "\n",
        "if pc.has_index(index_name):\n",
        "    desc = pc.describe_index(index_name)\n",
        "    if desc.dimension != 768:\n",
        "        print(f\"Index '{index_name}' has dimension {desc.dimension}, deleting to recreate with 768...\")\n",
        "        pc.delete_index(index_name)\n",
        "\n",
        "if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "\n",
        "index = pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12dea87",
      "metadata": {},
      "source": [
        "### Create the embedding model and the vector store\n",
        "\n",
        "I use Google's `gemini-embedding-001` to convert text into 768-dimensional vectors (reduced from the default 3072 via the `output_dimensionality` parameter). Then I wrap the Pinecone index with `PineconeVectorStore`, which provides a high-level LangChain interface for adding, deleting, and searching documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f81659d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    task_type=\"SEMANTIC_SIMILARITY\",\n",
        "    output_dimensionality=768,\n",
        ")\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3faa848e",
      "metadata": {},
      "source": [
        "## 3 - Manage the Vector Store\n",
        "\n",
        "### Add items\n",
        "\n",
        "I create 10 sample `Document` objects (each with `page_content` and `metadata`) and upsert them into Pinecone using `add_documents`. Each document receives a unique UUID so I can reference or delete it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2e47e937",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['1ba648b6-4642-456a-87e3-40629100f5fe',\n",
              " '3e294496-d0da-4764-aa43-6a82e870e224',\n",
              " '0fb150b4-5225-475c-a138-7d1e6fd25b91',\n",
              " 'fc386538-1a7c-41f7-8e6a-3fb5b747f659',\n",
              " '85fb5cc0-9237-4c9b-a2d2-ed2195ba519b',\n",
              " '1e4f120a-3496-4bb8-97d6-3497c736b89e',\n",
              " '5d4a145a-3e21-43e8-bd77-6ba0616d8158',\n",
              " 'd3805ffc-da90-411f-8029-b4697d18a91f',\n",
              " '72e537d9-636f-41bc-ad01-0317ed64be4d',\n",
              " 'dabddc3b-c3bd-448e-b221-3395eca0be61']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1, document_2, document_3, document_4, document_5,\n",
        "    document_6, document_7, document_8, document_9, document_10,\n",
        "]\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ba620f",
      "metadata": {},
      "source": [
        "### Delete items\n",
        "\n",
        "I can remove a document by its UUID. Here I delete the last one (`document_10`) which had a bad feeling about being deleted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "92538088",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.delete(ids=[uuids[-1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67ae8f9b",
      "metadata": {},
      "source": [
        "## 4 - Query the Vector Store\n",
        "\n",
        "### 4.1 Similarity search\n",
        "\n",
        "The simplest query: I pass a natural-language string and get back the **k** most similar documents. I also apply a metadata **filter** so only tweets are returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c68d5d85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"},\n",
        ")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f9874a",
      "metadata": {},
      "source": [
        "### 4.2 Similarity search with score\n",
        "\n",
        "Same as above but each result also returns a **similarity score** (cosine). This is useful when I want to set a threshold and discard low-confidence matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c96efe4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* [SIM=0.851] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\", k=1, filter={\"source\": \"news\"}\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd4bb39",
      "metadata": {},
      "source": [
        "### 4.3 Using the vector store as a Retriever\n",
        "\n",
        "LangChain's `as_retriever()` converts the vector store into a **Retriever** object that I can plug directly into chains and agents. Here I use `similarity_score_threshold` so only results above 0.4 are returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cb3383b8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='569d21a6-df70-4601-8f95-d0816622ea30', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 1, \"score_threshold\": 0.4},\n",
        ")\n",
        "retriever.invoke(\"Stealing from the bank is a crime\", filter={\"source\": \"news\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b066f9",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook I:\n",
        "1. Connected to Pinecone and created a serverless index (768 dims for Gemini embeddings).\n",
        "2. Initialized `GoogleGenerativeAIEmbeddings` (`gemini-embedding-001`) and `PineconeVectorStore`.\n",
        "3. Added and deleted documents.\n",
        "4. Queried the store via similarity search (with and without scores) and via a LangChain Retriever.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f136933e",
      "metadata": {},
      "source": [
        "# Notebook 02 - Retrieval-Augmented Generation (RAG) with Pinecone + Groq\n",
        "### By: JosuÃ© Hernandez\n",
        "\n",
        "In this notebook I implement a complete **RAG pipeline** using:\n",
        "- **Pinecone** as the vector database.\n",
        "- **Google Gemini** (free tier) for embeddings (`gemini-embedding-001`).\n",
        "- **Groq** (free tier) for the chat model (`llama-3.3-70b-versatile`).\n",
        "\n",
        "**What I will do step by step:**\n",
        "1. **Credentials** - I load API keys securely.\n",
        "2. **Index creation** - I create (or connect to) a Pinecone serverless index.\n",
        "3. **Document preparation** - I define sample documents and split them into chunks.\n",
        "4. **Indexing** - I embed the chunks and upsert them into Pinecone.\n",
        "5. **RAG chain** - I build a retrieval chain that fetches relevant context and generates an answer.\n",
        "6. **Query** - I ask questions and inspect both the answer and the retrieved sources.\n",
        "\n",
        "> **Note:** API keys are loaded from environment variables or prompted via `getpass`. I never hard-code secrets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "920daee9",
      "metadata": {},
      "source": [
        "## 1 - Credentials\n",
        "\n",
        "I load a `.env` file (if present) and make sure `GOOGLE_API_KEY`, `PINECONE_API_KEY`, and `GROQ_API_KEY` are set.\n",
        "\n",
        "- **Google Gemini** provides free-tier access to embedding models.\n",
        "- **Pinecone** offers a free starter tier for vector storage.\n",
        "- **Groq** provides free-tier access to LLMs like Llama 3.3 (30 RPM, very fast inference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e75f7e81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index: lab-rag-index | Cloud: aws | Region: us-east-1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API key: \")\n",
        "\n",
        "if not os.getenv(\"PINECONE_API_KEY\"):\n",
        "    os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Enter your Pinecone API key: \")\n",
        "\n",
        "if not os.getenv(\"GROQ_API_KEY\"):\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
        "\n",
        "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"lab-rag-index\")\n",
        "PINECONE_CLOUD = os.getenv(\"PINECONE_CLOUD\", \"aws\")\n",
        "PINECONE_REGION = os.getenv(\"PINECONE_REGION\", \"us-east-1\")\n",
        "\n",
        "print(f\"Index: {PINECONE_INDEX_NAME} | Cloud: {PINECONE_CLOUD} | Region: {PINECONE_REGION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac7347d",
      "metadata": {},
      "source": [
        "## 2 - Create / Connect to the Pinecone Index\n",
        "\n",
        "I instantiate the Pinecone client and create a **serverless index** if it does not already exist. I set the dimension to **768** because I use `gemini-embedding-001` with `output_dimensionality=768` (Matryoshka Representation Learning). I use **cosine** as the similarity metric.\n",
        "\n",
        "If the index already exists with a different dimension (e.g. 1536 from a previous OpenAI run), I automatically delete and recreate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b1ab7bf5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 768,\n",
              " 'index_fullness': 0.0,\n",
              " 'metric': 'cosine',\n",
              " 'namespaces': {'lab5': {'vector_count': 3}},\n",
              " 'total_vector_count': 3,\n",
              " 'vector_type': 'dense'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "EMBEDDING_DIMENSION = 768\n",
        "\n",
        "if pc.has_index(PINECONE_INDEX_NAME):\n",
        "    desc = pc.describe_index(PINECONE_INDEX_NAME)\n",
        "    if desc.dimension != EMBEDDING_DIMENSION:\n",
        "        print(f\"Index '{PINECONE_INDEX_NAME}' has dimension {desc.dimension}, deleting to recreate with {EMBEDDING_DIMENSION}...\")\n",
        "        pc.delete_index(PINECONE_INDEX_NAME)\n",
        "\n",
        "if not pc.has_index(PINECONE_INDEX_NAME):\n",
        "    pc.create_index(\n",
        "        name=PINECONE_INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIMENSION,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION),\n",
        "    )\n",
        "\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795c8d06",
      "metadata": {},
      "source": [
        "## 3 - Prepare the Documents\n",
        "\n",
        "I define three sample documents inline (no external files needed). Each `Document` object contains:\n",
        "- `page_content`: the actual text.\n",
        "- `metadata`: a dictionary with at least a `source` key for traceability.\n",
        "\n",
        "In a real-world scenario I would load documents from files, databases, or the web using LangChain **document loaders**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2c30e245",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 documents loaded.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "docs = [\n",
        "    Document(\n",
        "        page_content=(\n",
        "            \"Data Policy (example)\\n\\n\"\n",
        "            \"Retention:\\n\"\n",
        "            \"- Operational logs are kept for 90 days.\\n\"\n",
        "            \"- Backups are kept for 30 days.\\n\\n\"\n",
        "            \"Access:\\n\"\n",
        "            \"- Access to sensitive data requires role-based authorization.\\n\"\n",
        "            \"- All access events must be audited (who, when, what).\\n\\n\"\n",
        "            \"Security:\\n\"\n",
        "            \"- Secrets (API keys) must never be committed to version control.\\n\"\n",
        "            \"- Key rotation is recommended every 90 days.\"\n",
        "        ),\n",
        "        metadata={\"source\": \"data_policy\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=(\n",
        "            \"Product FAQ (example)\\n\\n\"\n",
        "            \"What is RAG?\\n\"\n",
        "            \"RAG (Retrieval-Augmented Generation) is a technique where a model retrieves \"\n",
        "            \"relevant information from a knowledge base (e.g. a vector store) and then \"\n",
        "            \"generates an answer using that context.\\n\\n\"\n",
        "            \"Why use a vector database?\\n\"\n",
        "            \"It enables semantic similarity search even when the query does not literally \"\n",
        "            \"match the stored text.\\n\\n\"\n",
        "            \"How to reduce hallucinations?\\n\"\n",
        "            \"- Constrain the answer to the retrieved context only.\\n\"\n",
        "            \"- Improve chunking strategy and document quality.\\n\"\n",
        "            \"- Apply similarity thresholds and/or re-ranking.\"\n",
        "        ),\n",
        "        metadata={\"source\": \"product_faq\"},\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=(\n",
        "            \"Quick Notes: LangChain (example)\\n\\n\"\n",
        "            \"Typical components in a RAG pipeline:\\n\"\n",
        "            \"1) Loader  2) Splitter  3) Embeddings  4) Vector store  5) Retriever  6) LLM\\n\\n\"\n",
        "            \"Best practices:\\n\"\n",
        "            \"- Keep metadata (source, path) for traceability.\\n\"\n",
        "            \"- Experiment with k, chunk_size, and chunk_overlap.\\n\"\n",
        "            \"- Log the retrieved context for debugging.\"\n",
        "        ),\n",
        "        metadata={\"source\": \"langchain_notes\"},\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(f\"{len(docs)} documents loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d142046",
      "metadata": {},
      "source": [
        "### Split documents into chunks\n",
        "\n",
        "I split the documents into smaller **chunks** so that:\n",
        "- Each chunk fits within the embedding model's token limit.\n",
        "- The retriever can return precise, relevant passages instead of entire documents.\n",
        "\n",
        "I use `RecursiveCharacterTextSplitter` which tries to split at natural boundaries (paragraphs, sentences) before falling back to character-level splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b0d066c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 chunks created from 3 documents.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'source': 'data_policy'}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"{len(splits)} chunks created from {len(docs)} documents.\")\n",
        "splits[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c5ef8f",
      "metadata": {},
      "source": [
        "## 4 - Indexing (Embed + Upsert)\n",
        "\n",
        "I initialize `GoogleGenerativeAIEmbeddings` (with `output_dimensionality=768`) and `PineconeVectorStore`, then I upsert the chunks. I use a **namespace** (`lab5`) to isolate this lab's data from anything else in the same index.\n",
        "\n",
        "To avoid duplicating vectors on repeated runs, I check whether the namespace already contains data and skip the upsert if so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3989ba85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace 'lab5' already has 3 vectors. Skipping upsert to avoid duplicates.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",\n",
        "    task_type=\"SEMANTIC_SIMILARITY\",\n",
        "    output_dimensionality=768,\n",
        ")\n",
        "\n",
        "NAMESPACE = \"lab5\"\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings, namespace=NAMESPACE)\n",
        "\n",
        "stats = index.describe_index_stats()\n",
        "existing = (stats.get(\"namespaces\") or {}).get(NAMESPACE, {}).get(\"vector_count\", 0)\n",
        "\n",
        "if existing and existing > 0:\n",
        "    print(\n",
        "        f\"Namespace '{NAMESPACE}' already has {existing} vectors. \"\n",
        "        \"Skipping upsert to avoid duplicates.\"\n",
        "    )\n",
        "else:\n",
        "    ids = vector_store.add_documents(documents=splits)\n",
        "    print(f\"Upserted {len(ids)} chunks into Pinecone (namespace={NAMESPACE}).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06a2f516",
      "metadata": {},
      "source": [
        "## 5 - Build the RAG Chain\n",
        "\n",
        "I build the RAG chain using **LCEL (LangChain Expression Language)**. It has two stages:\n",
        "\n",
        "1. **Retriever** - I fetch the top-k most relevant chunks from Pinecone for a given query.\n",
        "2. **Generation** - I pass the retrieved context plus the user's question to the LLM and produce an answer.\n",
        "\n",
        "I use `RunnablePassthrough` to pipe the input through and a helper function to format the retrieved documents into a single string for the prompt.\n",
        "\n",
        "My system prompt instructs the model to answer **only** from the provided context. If the context does not contain the answer, it must say so explicitly - this helps reduce hallucinations.\n",
        "\n",
        "I use **Groq** with **Llama 3.3 70B** as the chat model. Groq provides extremely fast inference on its free tier (30 requests/minute)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "02c039ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG chain ready.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful question-answering assistant.\\n\"\n",
        "            \"Answer the user's question using ONLY the CONTEXT below.\\n\"\n",
        "            \"If the context does not contain the answer, respond: \"\n",
        "            \"'I don't know based on the available context.'\\n\\n\"\n",
        "            \"CONTEXT:\\n{context}\",\n",
        "        ),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"RAG chain ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b32e115",
      "metadata": {},
      "source": [
        "## 6 - Query the RAG Chain\n",
        "\n",
        "### Question 1 - Answerable from context\n",
        "\n",
        "I ask a question whose answer exists in the indexed documents. The chain retrieves the relevant chunks, passes them as context to the LLM, and generates a grounded answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d699df6f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: What is the data retention policy?\n",
            "\n",
            "ANSWER: The data retention policy is as follows:\n",
            "- Operational logs are kept for 90 days.\n",
            "- Backups are kept for 30 days.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the data retention policy?\"\n",
        "\n",
        "answer = rag_chain.invoke(question)\n",
        "\n",
        "print(\"QUESTION:\", question)\n",
        "print(\"\\nANSWER:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6048afc5",
      "metadata": {},
      "source": [
        "### Inspect retrieved sources\n",
        "\n",
        "Transparency is key in RAG systems. Below I use the retriever directly to inspect which chunks were retrieved for the same question, so I can verify the answer is grounded in real data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b2afa70f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved chunks:\n",
            "\n",
            "[1] source=data_policy\n",
            "Data Policy (example)\n",
            "\n",
            "Retention:\n",
            "- Operational logs are kept for 90 days.\n",
            "- Backups are kept for 30 days.\n",
            "\n",
            "Access:\n",
            "- Access to sensitive data requires role-based authorization.\n",
            "- All access events must be audited (who, when, what).\n",
            "\n",
            "Security:\n",
            "- Secrets (API keys) must never be committed to version control.\n",
            "- Key rotation is recommended every 90 days.\n",
            "\n",
            "[2] source=product_faq\n",
            "Product FAQ (example)\n",
            "\n",
            "What is RAG?\n",
            "RAG (Retrieval-Augmented Generation) is a technique where a model retrieves relevant information from a knowledge base (e.g. a vector store) and then generates an answer using that context.\n",
            "\n",
            "Why use a vector database?\n",
            "It enables semantic similarity search even when the query does not literally match the stored text.\n",
            "\n",
            "How to reduce hallucinations?\n",
            "- Constrain the\n",
            "\n",
            "[3] source=langchain_notes\n",
            "Quick Notes: LangChain (example)\n",
            "\n",
            "Typical components in a RAG pipeline:\n",
            "1) Loader  2) Splitter  3) Embeddings  4) Vector store  5) Retriever  6) LLM\n",
            "\n",
            "Best practices:\n",
            "- Keep metadata (source, path) for traceability.\n",
            "- Experiment with k, chunk_size, and chunk_overlap.\n",
            "- Log the retrieved context for debugging.\n"
          ]
        }
      ],
      "source": [
        "retrieved_docs = retriever.invoke(question)\n",
        "\n",
        "print(\"Retrieved chunks:\")\n",
        "for i, doc in enumerate(retrieved_docs, start=1):\n",
        "    source = doc.metadata.get(\"source\", \"unknown\")\n",
        "    print(f\"\\n[{i}] source={source}\")\n",
        "    print(doc.page_content[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbf8b86",
      "metadata": {},
      "source": [
        "### Question 2 - Not answerable from context\n",
        "\n",
        "I now ask something that is **not** present in the indexed documents. The model should decline to answer rather than hallucinate, thanks to my system prompt constraint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7d81bcd5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: What is the support phone number?\n",
            "ANSWER: I don't know based on the available context.\n"
          ]
        }
      ],
      "source": [
        "question2 = \"What is the support phone number?\"\n",
        "answer2 = rag_chain.invoke(question2)\n",
        "\n",
        "print(\"QUESTION:\", question2)\n",
        "print(\"ANSWER:\", answer2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b40d08",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook I built a full RAG pipeline:\n",
        "\n",
        "| Step | Component | Tool |\n",
        "|------|-----------|------|\n",
        "| Document preparation | Inline `Document` objects | `langchain_core` |\n",
        "| Chunking | `RecursiveCharacterTextSplitter` | `langchain_text_splitters` |\n",
        "| Embedding | `GoogleGenerativeAIEmbeddings` (`gemini-embedding-001`, 768 dims) | `langchain_google_genai` |\n",
        "| Vector storage and retrieval | `PineconeVectorStore` | `langchain_pinecone` |\n",
        "| Answer generation | `ChatGroq` (`llama-3.3-70b-versatile`) via LCEL | `langchain_groq` |\n",
        "\n",
        "I constrained the LLM to answer **only** from the retrieved context, which helps reduce hallucinations."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
